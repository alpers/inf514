{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Dependencies",
   "id": "5e4fef83b3f8e9ca"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-21T15:25:51.415709Z",
     "start_time": "2024-12-21T15:25:51.412948Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import time\n",
    "import zipfile\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from gensim.models import word2vec, Word2Vec\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, precision_score, f1_score, classification_report\n",
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Dataset",
   "id": "1489e3526216c7bd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Load Dataset",
   "id": "a383900ede7e98d1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T15:25:53.405392Z",
     "start_time": "2024-12-21T15:25:51.429536Z"
    }
   },
   "cell_type": "code",
   "source": [
    "base_dir = \"../../\"\n",
    "data_dir = base_dir + \"data/\"\n",
    "model_dir = base_dir + \"model/\"\n",
    "\n",
    "with zipfile.ZipFile(data_dir + \"jobs.csv.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(data_dir)\n",
    "job_set = pd.read_csv(data_dir + \"jobs.csv\")\n",
    "user_set = pd.read_csv(data_dir + \"users.csv\")\n",
    "dataset = pd.read_csv(data_dir + \"dataset.csv\")\n",
    "work_history = pd.read_csv(data_dir + \"history.csv\")\n",
    "ranking_data = pd.read_csv(data_dir + \"ranking.csv\")\n",
    "jobs_segment_file = data_dir + \"jobs_segment.csv\"\n",
    "\n",
    "embedding_path = model_dir + \"embedding.pt\"\n",
    "model_path = model_dir + \"word2vec.model\"\n",
    "trained_model_path = model_dir + \"textCNN_ckpt.model\""
   ],
   "id": "9fd586caea83c4b4",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Text Preprocessing",
   "id": "8e1a36c8e569ca91"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T15:25:53.805631Z",
     "start_time": "2024-12-21T15:25:53.413356Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "punctuation = list('。，？！：%&~（）、；“”&|,.?!:%&~();\"\"#@【】/-\\'$+*`[]{}()')\n",
    "stop_words = stopwords.words(\"english\")\n",
    "stop_words.extend([\"n't\", \"wo\", \"'m\", \"'s\", \"'ve\", \"'d\", \"'ll\", \"``\", \"''\", \"--\", \"...\"])\n",
    "stop_words.extend(punctuation)\n",
    "wordnet_lematizer = WordNetLemmatizer()"
   ],
   "id": "17628eab09b0183f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/alper/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /Users/alper/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/alper/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /Users/alper/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T15:25:53.816197Z",
     "start_time": "2024-12-21T15:25:53.813457Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pretreatment(comment):\n",
    "    '''\n",
    "    remove punctuations, numbers and urls\n",
    "    lower case conversion\n",
    "    remove stop words\n",
    "    lemmatization\n",
    "    '''\n",
    "\n",
    "    token_words = word_tokenize(comment)\n",
    "    token_words = [w.lower() for w in token_words]\n",
    "    token_words = [w for w in token_words if w not in stop_words]\n",
    "    token_words = pos_tag(token_words)\n",
    "    cleaned_word = []\n",
    "    for word, tag in token_words:\n",
    "        if word.isdigit():\n",
    "            continue\n",
    "        if tag.startswith('NN'):\n",
    "            word_lematizer = wordnet_lematizer.lemmatize(word, pos='n')  # n for noun\n",
    "        elif tag.startswith('VB'):\n",
    "            word_lematizer = wordnet_lematizer.lemmatize(word, pos='v')  # v for verb\n",
    "        elif tag.startswith('JJ'):\n",
    "            word_lematizer = wordnet_lematizer.lemmatize(word, pos='a')  # a for adjective\n",
    "        elif tag.startswith('R'):\n",
    "            word_lematizer = wordnet_lematizer.lemmatize(word, pos='r')  # r for pronoun\n",
    "        else:\n",
    "            word_lematizer = wordnet_lematizer.lemmatize(word)\n",
    "        cleaned_word.append(word_lematizer)\n",
    "\n",
    "    return cleaned_word"
   ],
   "id": "655e7a6c354f490e",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Load Job File",
   "id": "b15c2e1956889440"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T15:35:53.557641Z",
     "start_time": "2024-12-21T15:25:53.841504Z"
    }
   },
   "cell_type": "code",
   "source": [
    "segment = []\n",
    "job_set = job_set.fillna(\"\")\n",
    "job_set[\"word\"] = job_set.Title + job_set.Description + job_set.Requirements\n",
    "for content in tqdm(job_set[\"word\"].values):\n",
    "    segment.append(pretreatment(content))\n",
    "job_set[\"text\"] = segment\n",
    "job_set.to_csv(jobs_segment_file, index=False)"
   ],
   "id": "17f24fc3a5392bb0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115684/115684 [09:52<00:00, 195.28it/s]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training",
   "id": "4c6f2ce5e9ae4652"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Training Classes and Functions",
   "id": "d8f409bbb3bbb55a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T15:35:53.595202Z",
     "start_time": "2024-12-21T15:35:53.584929Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Preprocess():\n",
    "    def __init__(self, sentences, sen_len, w2v_path):\n",
    "        '''\n",
    "        param: sentences: the list of corpus\n",
    "               sen_len: the max length of each sentence\n",
    "               w2v_path: the path storing word emnbedding model\n",
    "        '''\n",
    "\n",
    "        self.w2v_path = w2v_path\n",
    "        self.sentences = sentences\n",
    "        self.sen_len = sen_len\n",
    "        self.idx2word = []\n",
    "        self.word2idx = {}\n",
    "        self.embedding_matrix = []\n",
    "        if w2v_path:\n",
    "            self.embedding = Word2Vec.load(self.w2v_path)\n",
    "            self.embedding_dim = self.embedding.vector_size\n",
    "        else:\n",
    "            self.embedding = None\n",
    "            self.embedding_dim = None\n",
    "\n",
    "    def add_embedding(self, word):\n",
    "        self.word2idx[word] = len(self.word2idx)\n",
    "        self.idx2word.append(word)\n",
    "        if self.embedding_matrix is None:\n",
    "            vector = torch.zeros(100)\n",
    "            self.embedding_matrix = vector.unsqueeze(0)\n",
    "        else:\n",
    "            vector = torch.zeros(self.embedding_matrix.size(1))\n",
    "            self.embedding_matrix = torch.cat([self.embedding_matrix, vector.unsqueeze(0)], dim=0)\n",
    "        #vector = torch.empty(1, self.embedding_dim)\n",
    "        #torch.nn.init.uniform_(vector)\n",
    "\n",
    "    def make_embedding(self, load=True):\n",
    "        print(\"Get embedding ...\")\n",
    "        if load:\n",
    "            print(\"loading word2vec model ...\")\n",
    "            self.embedding = Word2Vec.load(self.w2v_path)\n",
    "            self.embedding_matrix = []\n",
    "            for word in self.embedding.wv.index_to_key:\n",
    "                self.word2idx[word] = len(self.word2idx)\n",
    "                self.idx2word.append(word)\n",
    "                self.embedding_matrix.append(self.embedding.wv[word])\n",
    "            self.embedding_matrix = torch.tensor(self.embedding_matrix)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        self.add_embedding(\"<PAD>\")\n",
    "        self.add_embedding(\"<UNK>\")\n",
    "        print(\"total words: {}\".format(len(self.embedding_matrix)))\n",
    "        return self.embedding_matrix\n",
    "\n",
    "    def pad_sentence(self, sentence):\n",
    "        if len(sentence) > self.sen_len:\n",
    "            sentence = sentence[:self.sen_len]\n",
    "        else:\n",
    "            pad_len = self.sen_len - len(sentence)\n",
    "            for _ in range(pad_len):\n",
    "                sentence.append(self.word2idx['<PAD>'])\n",
    "        assert len(sentence) == self.sen_len\n",
    "        return sentence\n",
    "\n",
    "    def sentence_word2idx(self):\n",
    "        '''\n",
    "        change words in sentences into idx in embedding_matrix\n",
    "        '''\n",
    "        sentence_list = []\n",
    "        for i, sen in enumerate(self.sentences):\n",
    "            sentence_idx = []\n",
    "            for word in sen:\n",
    "                if (word in self.word2idx.keys()):\n",
    "                    sentence_idx.append(self.word2idx[word])\n",
    "                else:\n",
    "                    sentence_idx.append(self.word2idx['<UNK>'])\n",
    "            sentence_idx = self.pad_sentence(sentence_idx)\n",
    "            sentence_list.append(sentence_idx)\n",
    "        return torch.LongTensor(sentence_list)\n",
    "\n",
    "    def labels_to_tensor(self, y):\n",
    "        return torch.LongTensor(y)\n",
    "\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, channels, kernel_size, pool_size, dim, method='max'):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.net1 = nn.Sequential(\n",
    "            nn.Conv2d(1, channels, kernel_size[0]),\n",
    "            nn.BatchNorm2d(channels),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(pool_size)\n",
    "        )\n",
    "        self.net2 = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, kernel_size[1]),\n",
    "            nn.BatchNorm2d(channels),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveMaxPool2d((1, dim))\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        if method is 'max':\n",
    "            self.pool = nn.AdaptiveMaxPool2d((1, dim))\n",
    "        elif method is 'mean':\n",
    "            self.pool = nn.AdaptiveAvgPool2d((1, dim))\n",
    "        else:\n",
    "            raise ValueError('method {} not exist'.format(method))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net1(x)\n",
    "        x = self.net2(x).squeeze(2)\n",
    "        x = self.pool(x).squeeze(1)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size, dropout):\n",
    "        super(MLP, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(input_size, input_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_size, output_size),\n",
    "            nn.Sigmoid()\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PJFNN(nn.Module):\n",
    "    def __init__(self, embedding, input_dim, channels=1, dropout=0.5, fix_embedding=True):\n",
    "        super(PJFNN, self).__init__()\n",
    "        self.dim = embedding.size(1)\n",
    "        self.user_dim = input_dim\n",
    "        self.channels = channels\n",
    "        self.embedding = nn.Embedding(embedding.size(0), embedding.size(1))\n",
    "        self.embedding.weight = nn.Parameter(embedding)\n",
    "        self.embedding.weight.requires_grad = False if fix_embedding else True\n",
    "        # self.emb = nn.Embedding.from_pretrained(\n",
    "        #    torch.from_numpy(np.load( os.path.join(args['dataset']['path'], 'emb.npy') )),\n",
    "        #    freeze=False,\n",
    "        #    padding_idx=0\n",
    "        #)\n",
    "\n",
    "        # self.geek_layer = TextCNN(\n",
    "        #     channels=args['dataset']['max_sent_num']['geek'],\n",
    "        #     kernel_size=[(5, 1), (3, 1)],\n",
    "        #     pool_size=(2, 1),\n",
    "        #     dim=dim,\n",
    "        #     method='max'\n",
    "        # )\n",
    "        self.user_layer = MLP(self.user_dim, 64, dropout=dropout)\n",
    "        self.linear_transform = nn.Linear(200, 64)\n",
    "        self.job_layer = TextCNN(\n",
    "            channels=self.channels,\n",
    "            kernel_size=[(5, 1), (5, 1)],\n",
    "            pool_size=(2, 1),\n",
    "            dim=200,\n",
    "            method='mean'\n",
    "        )\n",
    "\n",
    "        self.mlp = MLP(\n",
    "            input_size=128,\n",
    "            output_size=1,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "    def forward(self, job, user):\n",
    "        job = self.embedding(job)\n",
    "        job = job.unsqueeze(1)\n",
    "        job = self.job_layer(job)\n",
    "        user = self.user_layer(user)\n",
    "        job = self.linear_transform(job)\n",
    "        x = torch.cat((user, job), dim=1)\n",
    "        x = self.mlp(x).squeeze(1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PJFNN_LSTM(nn.Module):\n",
    "    def __init__(self, embedding, input_dim, hidden_dim=256, num_layers=1, dropout=0., fix_embedding=True):\n",
    "        super(PJFNN_LSTM, self).__init__()\n",
    "        self.embedding_dim = embedding.size(1)\n",
    "        self.user_dim = input_dim\n",
    "        self.embedding = nn.Embedding(embedding.size(0), embedding.size(1))\n",
    "        self.embedding.weight = nn.Parameter(embedding)\n",
    "        self.embedding.weight.requires_grad = False if fix_embedding else True\n",
    "        # self.emb = nn.Embedding.from_pretrained(\n",
    "        #    torch.from_numpy(np.load( os.path.join(args['dataset']['path'], 'emb.npy') )),\n",
    "        #    freeze=False,\n",
    "        #    padding_idx=0\n",
    "        #)\n",
    "\n",
    "        # self.geek_layer = TextCNN(\n",
    "        #     channels=args['dataset']['max_sent_num']['geek'],\n",
    "        #     kernel_size=[(5, 1), (3, 1)],\n",
    "        #     pool_size=(2, 1),\n",
    "        #     dim=dim,\n",
    "        #     method='max'\n",
    "        # )\n",
    "        self.user_layer = MLP(self.user_dim, 64, dropout=dropout)\n",
    "        self.linear_transform = nn.Linear(256, 64)\n",
    "        self.job_layer = nn.LSTM(self.embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True,\n",
    "                                 dropout=dropout)\n",
    "\n",
    "        self.classifier = nn.Sequential(nn.Dropout(dropout), nn.Linear(128, 1), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, job, user):\n",
    "        job = self.embedding(job)\n",
    "        job, _ = self.job_layer(job, None)\n",
    "        job = job[:, -1, :]\n",
    "        user = self.user_layer(user)\n",
    "        job = self.linear_transform(job)\n",
    "        x = torch.cat((user, job), dim=1)\n",
    "        x = self.classifier(x).squeeze(1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class JobUserDataset(data.Dataset):\n",
    "    '''\n",
    "    Expected data shape like:(data_num, data_len)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, job, user, label):\n",
    "        self.job = job\n",
    "        self.user = user\n",
    "        self.label = label\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.label is None: return self.job[idx], self.user[idx]\n",
    "        return self.job[idx], self.user[idx], self.label[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.job), len(self.user), len(self.label) if self.label is not None else float('inf'))"
   ],
   "id": "11b3d6e825adde4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:98: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
      "<>:100: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
      "<>:98: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
      "<>:100: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
      "/var/folders/38/xlfphpn930qcty0ghdcwlsfr0000gn/T/ipykernel_84434/2569812986.py:98: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
      "  if method is 'max':\n",
      "/var/folders/38/xlfphpn930qcty0ghdcwlsfr0000gn/T/ipykernel_84434/2569812986.py:100: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
      "  elif method is 'mean':\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T15:35:53.623778Z",
     "start_time": "2024-12-21T15:35:53.617342Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def training(batch_size, n_epoch, lr, train, valid, model, device, model_name, model_dir):\n",
    "    # summary model parameters\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    print(\"\\nstart training, total parameter:{}, trainable:{}\\n\".format(total, trainable))\n",
    "    #model.cuda()\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    criterion = nn.BCELoss()\n",
    "    t_batch = len(train)\n",
    "    print(\"batch size:{}, epoch:{}, t_batch:{}\".format(batch_size, n_epoch, t_batch))\n",
    "    v_batch = len(valid)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    total_loss, total_acc, best_acc = 0, 0, 0\n",
    "    train_losses, val_losses = [], []\n",
    "    train_acc, val_acc = [], []\n",
    "    pred_label = []\n",
    "    y_label = []\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        start_time = time.time()\n",
    "        total_loss, total_acc = 0.0, 0\n",
    "        # training\n",
    "        print(\"epoch-{} training has started.\".format(epoch))\n",
    "        for i, (jobs, users, labels) in enumerate(train):\n",
    "            jobs = jobs.to(device)\n",
    "            users = users.to(torch.float32)\n",
    "            users = users.to(device)\n",
    "            labels = labels.to(device)\n",
    "            labels = labels.to(torch.float32)\n",
    "            model.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(jobs, users)\n",
    "            #print(f\"Raw model outputs: {outputs}\")\n",
    "            #outputs = torch.sigmoid(outputs)\n",
    "            #print(f\"Sigmoid model outputs: {outputs}\")\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            pred_label.extend([0 if i < 0.5 else 1 for i in list(outputs.cpu().detach().numpy())])\n",
    "            y_label.extend(list(labels.cpu().detach().numpy()))\n",
    "        train_losses.append(total_loss / t_batch)\n",
    "        train_acc.append(accuracy_score(y_label, pred_label))\n",
    "        print('[ Epoch{}: {}/{}] '.format(\n",
    "            epoch + 1, i + 1, t_batch))\n",
    "        print('\\nTrain | Loss:{:.5f} Time:{:.6f}'.format(total_loss / t_batch, time.time() - start_time))\n",
    "\n",
    "        print(\"epoch-{} evaluation has started.\".format(epoch))\n",
    "        # evaluation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred_label = []\n",
    "            y_label = []\n",
    "            total_loss, total_acc = 0.0, 0\n",
    "            for i, (jobs, users, labels) in enumerate(valid):\n",
    "                jobs = jobs.to(device)\n",
    "                users = users.to(torch.float32)\n",
    "                users = users.to(device)\n",
    "                labels = labels.to(torch.float32)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(jobs, users)\n",
    "                #outputs = torch.sigmoid(outputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                total_loss += loss.item()\n",
    "                pred_label.extend([0 if i < 0.5 else 1 for i in list(outputs.cpu().detach().numpy())])\n",
    "                y_label.extend(list(labels.cpu().detach().numpy()))\n",
    "            # print('\\nVal | Loss:{:.5f} Time:{:.6f}'.format(total_loss/v_batch, time.time()-start_time))\n",
    "            val_losses.append(total_loss / v_batch)\n",
    "            total_acc = accuracy_score(y_label, pred_label)\n",
    "            val_acc.append(total_acc)\n",
    "            print('\\nVal | ACC:{:.5f} Time:{:.6f}'.format(total_acc, time.time() - start_time))\n",
    "            if total_acc > best_acc:\n",
    "                best_acc = total_acc\n",
    "                torch.save(model, \"{}/{}_ckpt.model\".format(model_dir, model_name))\n",
    "                print('save model with acc {:.3f}'.format(total_acc))\n",
    "\n",
    "        print(\"epoch-{} has finished.\".format(epoch))\n",
    "        print('------------------------------------------------------')\n",
    "        model.train()\n",
    "    return train_losses, val_losses, train_acc, val_acc\n",
    "\n",
    "\n",
    "def test(model, loader):\n",
    "    predictions = []\n",
    "    y_labels = []\n",
    "    pred = []\n",
    "    model.eval()\n",
    "    for i, (jobs, users, labels) in enumerate(loader):\n",
    "        jobs = jobs.to(device)\n",
    "        users = users.to(device, dtype=torch.float32)\n",
    "        labels = labels.to(device, dtype=torch.float32)\n",
    "        outputs = model(jobs, users)\n",
    "        pred.extend(list(outputs.cpu().detach().numpy()))\n",
    "        predictions.extend([0 if i < 0.5 else 1 for i in list(outputs.cpu().detach().numpy())])\n",
    "        y_labels.extend(list(labels.cpu().detach().numpy()))\n",
    "        report = classification_report(y_labels, predictions, digits=4)\n",
    "    report = report.splitlines()\n",
    "    columns = ['class'] + report[0].split()\n",
    "    col_1, col_2, col_3, col_4, col_5 = [], [], [], [], []\n",
    "    for row in report[1:]:\n",
    "        if len(row.split()) != 0:\n",
    "            row = row.split()\n",
    "            if len(row) < 5:\n",
    "                col_1.append(row[0])\n",
    "                col_2.append('')\n",
    "                col_3.append('')\n",
    "                col_4.append(row[1])\n",
    "                col_5.append(row[2])\n",
    "            elif len(row) > 5:\n",
    "                col_1.append(row[0] + ' ' + row[1])\n",
    "                col_2.append(row[2])\n",
    "                col_3.append(row[3])\n",
    "                col_4.append(row[4])\n",
    "                col_5.append(row[5])\n",
    "            else:\n",
    "                col_1.append(row[0])\n",
    "                col_2.append(row[1])\n",
    "                col_3.append(row[2])\n",
    "                col_4.append(row[3])\n",
    "                col_5.append(row[4])\n",
    "    result = pd.DataFrame()\n",
    "    col_1.append(\"overall\")\n",
    "    col_2.append(precision_score(y_labels, predictions))\n",
    "    col_3.append(recall_score(y_labels, predictions))\n",
    "    col_4.append(f1_score(y_labels, predictions))\n",
    "    col_5.append(roc_auc_score(y_labels, pred))\n",
    "    result[columns[0]] = col_1\n",
    "    result[columns[1]] = col_2\n",
    "    result[columns[2]] = col_3\n",
    "    result[columns[3]] = col_4\n",
    "    result[columns[4]] = col_5\n",
    "    print(\"——————Test——————\")\n",
    "    #     print(result)\n",
    "    return result"
   ],
   "id": "bba6ae560b92329",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Train Model",
   "id": "2346f076b0fe004"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T15:39:56.762997Z",
     "start_time": "2024-12-21T15:35:53.626290Z"
    }
   },
   "cell_type": "code",
   "source": [
    "w2v_model = word2vec.Word2Vec(job_set.text.values, vector_size=200, window=5, min_count=2, workers=8, epochs=10, sg=1)\n",
    "w2v_model.save(model_path)\n",
    "w2v_model = Word2Vec.load(model_path)\n",
    "word_history_tf = TfidfVectorizer(analyzer='word', ngram_range=(1, 2), min_df=1, max_features=50, stop_words='english')\n",
    "word_history_tf_matrix = word_history_tf.fit_transform(work_history.groupby(\"UserID\").JobTitle.sum().values)"
   ],
   "id": "9082f176c95f5a9d",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Build Dataset",
   "id": "e746f7d32faa430c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T15:40:05.154742Z",
     "start_time": "2024-12-21T15:39:56.778004Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_user = user_set[user_set.Split == \"Train\"].UserID.values\n",
    "test_user = user_set[user_set.Split == \"Test\"].UserID.values\n",
    "train_data = dataset[dataset.UserID.isin(train_user)]\n",
    "test_data = dataset[dataset.UserID.isin(test_user)]\n",
    "job_set = pd.read_csv(jobs_segment_file)\n",
    "text = []\n",
    "for i in job_set.text:\n",
    "    temp = i[1:-1].split(',')\n",
    "    text.append([t.strip()[1:-1] for t in temp])\n",
    "job_set[\"text\"] = text"
   ],
   "id": "a7ba98f108cc8c70",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T15:40:46.029934Z",
     "start_time": "2024-12-21T15:40:05.159827Z"
    }
   },
   "cell_type": "code",
   "source": [
    "groups = train_data.groupby(\"UserID\")\n",
    "job_train = []\n",
    "user_train = np.zeros((1, 58))\n",
    "Y_train = []\n",
    "for u_id, group in tqdm(groups):\n",
    "    user = user_set[user_set.UserID == u_id][\n",
    "        [\"DegreeType\", \"WorkHistoryCount\", \"TotalYearsExperience\", \"CurrentlyEmployed\",\n",
    "         \"ManagedOthers\", \"ManagedHowMany\"]]\n",
    "    u_idx = user.index.values[0]\n",
    "    user_feature = np.concatenate((user.values, word_history_tf_matrix[u_idx, :].toarray()), axis=1)\n",
    "    job_id_list = group.JobID.values\n",
    "    jobs = job_set[job_set.JobID.isin(job_id_list)]\n",
    "    job_train.extend(jobs.text.values.tolist())\n",
    "    user_feature = user_feature.repeat(len(jobs), axis=0)\n",
    "    user_feature = np.concatenate((user_feature, group[[\"State\", \"City\"]].values), axis=1)\n",
    "    user_train = np.concatenate((user_train, user_feature), axis=0)\n",
    "    Y_train.extend(group.label.values.tolist())"
   ],
   "id": "ca5085f83101de03",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18486/18486 [00:40<00:00, 452.74it/s]\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T15:40:46.390576Z",
     "start_time": "2024-12-21T15:40:46.041790Z"
    }
   },
   "cell_type": "code",
   "source": [
    "groups = test_data.groupby(\"UserID\")\n",
    "job_test = []\n",
    "user_test = np.zeros((1, 58))\n",
    "Y_test = []\n",
    "for u_id, group in tqdm(groups):\n",
    "    user = user_set[user_set.UserID == u_id][\n",
    "        [\"DegreeType\", \"WorkHistoryCount\", \"TotalYearsExperience\", \"CurrentlyEmployed\",\n",
    "         \"ManagedOthers\", \"ManagedHowMany\"]]\n",
    "    u_idx = user.index.values[0]\n",
    "    user_feature = np.concatenate((user.values, word_history_tf_matrix[u_idx, :].toarray()), axis=1)\n",
    "    job_id_list = group.JobID.values\n",
    "    jobs = job_set[job_set.JobID.isin(job_id_list)]\n",
    "    job_test.extend(jobs.text.values.tolist())\n",
    "    user_feature = user_feature.repeat(len(jobs), axis=0)\n",
    "    user_feature = np.concatenate((user_feature, group[[\"State\", \"City\"]].values), axis=1)\n",
    "    user_test = np.concatenate((user_test, user_feature), axis=0)\n",
    "    Y_test.extend(group.label.values.tolist())"
   ],
   "id": "42bdf9c7720a8aea",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 260/260 [00:00<00:00, 760.75it/s]\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T15:40:51.374806Z",
     "start_time": "2024-12-21T15:40:46.402173Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_len = len(job_train)\n",
    "job_train.extend(job_test)\n",
    "Y_train.extend(Y_test)\n",
    "sen_len = 200\n",
    "preprocess = Preprocess(job_train, sen_len, w2v_path=model_path)\n",
    "embedding = preprocess.make_embedding(load=True)\n",
    "x = preprocess.sentence_word2idx()\n",
    "y = preprocess.labels_to_tensor(Y_train)\n",
    "torch.save(embedding, embedding_path)"
   ],
   "id": "75e55acacd065b81",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get embedding ...\n",
      "loading word2vec model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/38/xlfphpn930qcty0ghdcwlsfr0000gn/T/ipykernel_84434/2569812986.py:44: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:281.)\n",
      "  self.embedding_matrix = torch.tensor(self.embedding_matrix)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words: 108123\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T15:40:51.417432Z",
     "start_time": "2024-12-21T15:40:51.406240Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_x = x[:70000]\n",
    "train_y = y[:70000]\n",
    "val_x = x[70000:70680]\n",
    "val_y = y[70000:70680]\n",
    "test_x = x[70680:]\n",
    "test_y = y[70680:]\n",
    "train_user = torch.from_numpy(user_train[1:70001])\n",
    "val_user = torch.from_numpy(user_train[70001:])\n",
    "test_user = torch.from_numpy(user_test[1:])\n",
    "\n",
    "train_user = torch.nan_to_num(train_user)\n",
    "val_user = torch.nan_to_num(val_user)\n",
    "test_user = torch.nan_to_num(test_user)\n",
    "\n",
    "train_dataset = JobUserDataset(train_x, train_user, train_y)\n",
    "val_dataset = JobUserDataset(val_x, val_user, val_y)\n",
    "test_dataset = JobUserDataset(test_x, test_user, test_y)\n",
    "# torch.save(train_dataset,\"train.dataset\")\n",
    "# torch.save(val_dataset,\"val.dataset\")\n",
    "# torch.save(test_dataset, \"test.dataset\")\n",
    "\n",
    "# train_dataset = torch.load(\"train.dataset\")\n",
    "# val_dataset = torch.load(\"val.dataset\")\n",
    "# test_dataset = torch.load(\"test.dataset\")\n",
    "# embedding = torch.load(embedding_path)\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n"
   ],
   "id": "ed64703089e4d313",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Train and Test",
   "id": "615653c4b8a1feaf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T15:45:44.121977Z",
     "start_time": "2024-12-21T15:40:51.445063Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fix_embedding = False\n",
    "input_dim = train_dataset[0][1].shape[0]\n",
    "model = PJFNN(embedding, input_dim, dropout=0.7, channels=32, fix_embedding=fix_embedding)\n",
    "#epoch = 10\n",
    "epoch = 2\n",
    "lr = 0.0005\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_built() else torch.device(\"cpu\")\n",
    "\n",
    "train_losses, val_losses, train_acc, val_acc = training(batch_size, epoch, lr, train_loader, val_loader, model, device,\n",
    "                                                        \"textCNN\", model_dir)"
   ],
   "id": "5612254c251952b6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "start training, total parameter:21666775, trainable:21666775\n",
      "\n",
      "batch size:32, epoch:2, t_batch:2188\n",
      "epoch-0 training has started.\n",
      "[ Epoch1: 2188/2188] \n",
      "\n",
      "Train | Loss:0.69497 Time:146.327729\n",
      "epoch-0 evaluation has started.\n",
      "\n",
      "Val | ACC:0.48653 Time:146.668194\n",
      "save model with acc 0.487\n",
      "epoch-0 has finished.\n",
      "------------------------------------------------------\n",
      "epoch-1 training has started.\n",
      "[ Epoch2: 2188/2188] \n",
      "\n",
      "Train | Loss:0.69360 Time:144.505541\n",
      "epoch-1 evaluation has started.\n",
      "\n",
      "Val | ACC:0.50449 Time:144.811056\n",
      "save model with acc 0.504\n",
      "epoch-1 has finished.\n",
      "------------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T15:45:44.835308Z",
     "start_time": "2024-12-21T15:45:44.164824Z"
    }
   },
   "cell_type": "code",
   "source": "test(torch.load(trained_model_path), test_loader)",
   "id": "19437ed99c0c754c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/38/xlfphpn930qcty0ghdcwlsfr0000gn/T/ipykernel_84434/2852594504.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test(torch.load(trained_model_path), test_loader)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "——————Test——————\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "          class precision    recall  f1-score   support\n",
       "0           0.0    0.5032    0.9119    0.6485       522\n",
       "1           1.0    0.5208    0.0962    0.1623       520\n",
       "2      accuracy                        0.5048      1042\n",
       "3     macro avg    0.5120    0.5040    0.4054      1042\n",
       "4  weighted avg    0.5120    0.5048    0.4059      1042\n",
       "5       overall  0.520833  0.096154  0.162338  0.522985"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5032</td>\n",
       "      <td>0.9119</td>\n",
       "      <td>0.6485</td>\n",
       "      <td>522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5208</td>\n",
       "      <td>0.0962</td>\n",
       "      <td>0.1623</td>\n",
       "      <td>520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>accuracy</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.5048</td>\n",
       "      <td>1042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>macro avg</td>\n",
       "      <td>0.5120</td>\n",
       "      <td>0.5040</td>\n",
       "      <td>0.4054</td>\n",
       "      <td>1042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>weighted avg</td>\n",
       "      <td>0.5120</td>\n",
       "      <td>0.5048</td>\n",
       "      <td>0.4059</td>\n",
       "      <td>1042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>overall</td>\n",
       "      <td>0.520833</td>\n",
       "      <td>0.096154</td>\n",
       "      <td>0.162338</td>\n",
       "      <td>0.522985</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Recommendation",
   "id": "3259c57ccc406445"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T15:45:45.684351Z",
     "start_time": "2024-12-21T15:45:44.874796Z"
    }
   },
   "cell_type": "code",
   "source": [
    "groups = ranking_data.groupby(\"UserID\")\n",
    "job_rank = []\n",
    "user_rank = np.zeros((1, 58))\n",
    "for u_id, group in tqdm(groups):\n",
    "    user = user_set[user_set.UserID == u_id][\n",
    "        [\"DegreeType\", \"WorkHistoryCount\", \"TotalYearsExperience\", \"CurrentlyEmployed\", \"ManagedOthers\",\n",
    "         \"ManagedHowMany\"]]\n",
    "    u_idx = user.index.values[0]\n",
    "    user_feature = np.concatenate((user.values, word_history_tf_matrix[u_idx, :].toarray()), axis=1)\n",
    "    job_id_list = group.JobID.values\n",
    "    jobs = job_set[job_set.JobID.isin(job_id_list)]\n",
    "    job_rank.extend(jobs.text.values.tolist())\n",
    "    user_feature = user_feature.repeat(len(jobs), axis=0)\n",
    "    user_feature = np.concatenate((user_feature, group[[\"State\", \"City\"]].values), axis=1)\n",
    "    user_rank = np.concatenate((user_rank, user_feature), axis=0)"
   ],
   "id": "91a659bc230c93db",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 260/260 [00:00<00:00, 327.05it/s]\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T15:45:48.868452Z",
     "start_time": "2024-12-21T15:45:45.700128Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sen_len = 200\n",
    "preprocess = Preprocess(job_rank, sen_len, w2v_path=model_path)\n",
    "embedding = preprocess.make_embedding(load=True)\n",
    "rank_x = preprocess.sentence_word2idx()\n",
    "rank_dataset = JobUserDataset(rank_x, user_rank, None)\n",
    "rank_loader = DataLoader(dataset=rank_dataset, batch_size=100, shuffle=False)\n",
    "num_user = len(ranking_data.UserID.unique())\n",
    "m = torch.load(trained_model_path)"
   ],
   "id": "8ecdbff6b2a06eae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get embedding ...\n",
      "loading word2vec model ...\n",
      "total words: 108123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/38/xlfphpn930qcty0ghdcwlsfr0000gn/T/ipykernel_84434/3599805002.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  m = torch.load(trained_model_path)\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Evaluation",
   "id": "87f6a26737e651f6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Evaluation Functions",
   "id": "9884b9eaee201ea5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T15:45:48.893301Z",
     "start_time": "2024-12-21T15:45:48.890252Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test_hit_rate(model, k, rank_loader):\n",
    "    hit = 0\n",
    "    num_users = 0\n",
    "    model.eval()\n",
    "    for jobs, users in rank_loader:\n",
    "        jobs = jobs.to(device)\n",
    "        users = users.to(torch.float32)\n",
    "        users = users.to(device)\n",
    "        outputs = model(jobs, users)\n",
    "        pred = outputs.cpu().detach().numpy()\n",
    "        if pred.size == 0:\n",
    "            continue\n",
    "        a = -np.sort(-pred)\n",
    "        matches = np.argwhere(a == pred[0])\n",
    "        if matches.size == 0:\n",
    "            continue\n",
    "        idx = np.argwhere(a == pred[0])[0][0]\n",
    "        if idx <= k - 1:\n",
    "            hit += 1\n",
    "        num_users += 1\n",
    "\n",
    "    return hit / num_user\n",
    "\n",
    "\n",
    "def test_ndcg(model, k, rank_loader):\n",
    "    ndcg_sum = 0\n",
    "    num_users = 0\n",
    "    model.eval()\n",
    "\n",
    "    for jobs, users in rank_loader:\n",
    "        jobs = jobs.to(device)\n",
    "        users = users.to(torch.float32).to(device)\n",
    "        outputs = model(jobs, users)\n",
    "        predictions = outputs.cpu().detach().numpy()\n",
    "\n",
    "        if len(predictions) == 0:\n",
    "            continue\n",
    "\n",
    "        relevance = [1 if pred == predictions[0] else 0 for pred in predictions]\n",
    "        sorted_relevance = [relevance[i] for i in np.argsort(-predictions)]\n",
    "\n",
    "        # DCG\n",
    "        dcg = 0\n",
    "        for i in range(k):\n",
    "            if i < len(sorted_relevance):\n",
    "                dcg += sorted_relevance[i] / np.log2(i + 2)  # log2(i + 2) because of 1-based indexing\n",
    "\n",
    "        # IDCG\n",
    "        ideal_relevance = sorted(sorted_relevance, reverse=True)\n",
    "        idcg = 0\n",
    "        for i in range(k):\n",
    "            if i < len(ideal_relevance):\n",
    "                idcg += ideal_relevance[i] / np.log2(i + 2)\n",
    "\n",
    "        # NDCG\n",
    "        ndcg = dcg / idcg if idcg > 0 else 0\n",
    "        ndcg_sum += ndcg\n",
    "        num_users += 1\n",
    "\n",
    "    return ndcg_sum / num_users if num_users > 0 else 0"
   ],
   "id": "6477e1e6ae2d1482",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Evaluation Results",
   "id": "15b57e044273c0cf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T15:46:11.844318Z",
     "start_time": "2024-12-21T15:45:48.896844Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ndcg_5 = test_ndcg(model, 5, rank_loader)\n",
    "ndcg_10 = test_ndcg(model, 10, rank_loader)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 30)\n",
    "print(f\"{'Metric':<15}{'Score':<10}\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"{'nDCG@5':<15}{ndcg_5:<10.4f}\")\n",
    "print(f\"{'nDCG@10':<15}{ndcg_10:<10.4f}\")\n",
    "print(\"-\" * 30)"
   ],
   "id": "4ced7f79eca243f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------\n",
      "Metric         Score     \n",
      "------------------------------\n",
      "nDCG@5         0.1980    \n",
      "nDCG@10        0.2101    \n",
      "------------------------------\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T15:46:34.065746Z",
     "start_time": "2024-12-21T15:46:11.871954Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hits_5 = test_hit_rate(model, 5, rank_loader)\n",
    "hits_10 = test_hit_rate(model, 10, rank_loader)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 30)\n",
    "print(f\"{'Metric':<15}{'Score':<10}\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"{'hits@5':<15}{hits_5:<10.4f}\")\n",
    "print(f\"{'hits@10':<15}{hits_10:<10.4f}\")\n",
    "print(\"-\" * 30)"
   ],
   "id": "5bc784706f8876bb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------\n",
      "Metric         Score     \n",
      "------------------------------\n",
      "hits@5         0.2308    \n",
      "hits@10        0.2692    \n",
      "------------------------------\n"
     ]
    }
   ],
   "execution_count": 22
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
